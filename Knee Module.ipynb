{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ead799",
   "metadata": {},
   "source": [
    "#### Immport modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309dda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca5b33",
   "metadata": {},
   "source": [
    "#### Read images creating a dataframe  and class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebeea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df lenght:  5778   test_df length:  1656   valid_df length:  826\n",
      "The number of classes in the dataset is:  5\n",
      "            CLASS               IMAGE COUNT \n",
      "           Doubtful                1046     \n",
      "           Healthy                 2286     \n",
      "           Minimal                 1516     \n",
      "           Moderate                 757     \n",
      "            Severe                  173     \n",
      "Healthy  has the most images=  2286   Severe  has the least images=  173\n",
      "average height=  224  average width=  224 aspect ratio=  1.0\n"
     ]
    }
   ],
   "source": [
    "train_path=r'C:\\Users\\gihan\\Desktop\\ResearchProject\\Dataset\\train'\n",
    "test_path=r'C:\\Users\\gihan\\Desktop\\ResearchProject\\Dataset\\test'\n",
    "valid_path=r'C:\\Users\\gihan\\Desktop\\ResearchProject\\Dataset\\val'\n",
    "list_of_classes=['Healthy', 'Doubtful', 'Minimal', 'Moderate', 'Severe']\n",
    "for d in [train_path, test_path, valid_path]:\n",
    "    filepaths = []\n",
    "    labels=[] \n",
    "    classlist=os.listdir(d)   \n",
    "    for klass in classlist:\n",
    "        intklass=int(klass)\n",
    "        label=list_of_classes[intklass]\n",
    "        classpath=os.path.join(d, klass)\n",
    "        flist=os.listdir(classpath)        \n",
    "        for f in flist:\n",
    "            fpath=os.path.join(classpath,f)\n",
    "            filepaths.append(fpath)\n",
    "            labels.append(label)\n",
    "    Fseries=pd.Series(filepaths, name='filepaths')\n",
    "    Lseries=pd.Series(labels, name='labels')        \n",
    "    pdf=pd.concat([Fseries, Lseries], axis=1)\n",
    "    if d == test_path:\n",
    "        test_df=pdf\n",
    "    elif d == valid_path:\n",
    "        valid_df=pdf\n",
    "    else:\n",
    "        train_df=pdf\n",
    "print('train_df lenght: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))\n",
    "\n",
    "# get the number of classes and the images count for each class in train_df\n",
    "classes=sorted(list(train_df['labels'].unique()))\n",
    "class_count = len(classes)\n",
    "print('The number of classes in the dataset is: ', class_count)\n",
    "groups=train_df.groupby('labels')\n",
    "print('{0:^30s} {1:^13s}'.format('CLASS', 'IMAGE COUNT'))\n",
    "countlist=[]\n",
    "classlist=[]\n",
    "for label in sorted(list(train_df['labels'].unique())):\n",
    "    group=groups.get_group(label)\n",
    "    countlist.append(len(group))\n",
    "    classlist.append(label)\n",
    "    print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n",
    "\n",
    "# get the classes with the minimum and maximum number of train images\n",
    "max_value=np.max(countlist)\n",
    "max_index=countlist.index(max_value)\n",
    "max_class=classlist[max_index]\n",
    "min_value=np.min(countlist)\n",
    "min_index=countlist.index(min_value)\n",
    "min_class=classlist[min_index]\n",
    "print(max_class, ' has the most images= ',max_value, ' ', min_class, ' has the least images= ', min_value)\n",
    "\n",
    "# lets get the average height and width of a sample of the train images\n",
    "ht=0\n",
    "wt=0\n",
    "\n",
    "# select 100 random samples of train_df\n",
    "train_df_sample=train_df.sample(n=100, random_state=123,axis=0)\n",
    "for i in range (len(train_df_sample)):\n",
    "    fpath=train_df_sample['filepaths'].iloc[i]\n",
    "    img=plt.imread(fpath)\n",
    "    shape=img.shape\n",
    "    ht += shape[0]\n",
    "    wt += shape[1]\n",
    "print('average height= ', ht//100, ' average width= ', wt//100, 'aspect ratio= ', ht/wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c32982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy 2286\n",
      "Moderate 757\n",
      "Severe 173\n"
     ]
    }
   ],
   "source": [
    "drop_classes = ['Minimal', 'Doubtful']\n",
    "\n",
    "train_df = train_df[~train_df['labels'].isin(drop_classes)]\n",
    "valid_df = valid_df[~valid_df['labels'].isin(drop_classes)]  \n",
    "test_df = test_df[~test_df['labels'].isin(drop_classes)]\n",
    "\n",
    "# Update the list of classes \n",
    "list_of_classes = [c for c in list_of_classes if c not in drop_classes]\n",
    "\n",
    "# Re-calculate the total classes\n",
    "class_count = len(list_of_classes)\n",
    "\n",
    "# Re-count the images per class\n",
    "groups = train_df.groupby('labels')  \n",
    "for label in list_of_classes:\n",
    "    group = groups.get_group(label) \n",
    "    print(label, len(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8e51d",
   "metadata": {},
   "source": [
    "#### Triming train_df  no class has more than 500 image samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1388813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after trimming, the maximum samples in any class is now  500  and the minimum samples in any class is  173\n"
     ]
    }
   ],
   "source": [
    "def trim(df, max_samples, min_samples, column):\n",
    "    df=df.copy()\n",
    "    groups=df.groupby(column)    \n",
    "    trimmed_df = pd.DataFrame(columns = df.columns)\n",
    "    groups=df.groupby(column)\n",
    "    for label in df[column].unique(): \n",
    "        group=groups.get_group(label)\n",
    "        count=len(group)    \n",
    "        if count > max_samples:\n",
    "            sampled_group=group.sample(n=max_samples, random_state=123,axis=0)\n",
    "            trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n",
    "        else:\n",
    "            if count>=min_samples:\n",
    "                sampled_group=group        \n",
    "                trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n",
    "    print('after trimming, the maximum samples in any class is now ',max_samples, ' and the minimum samples in any class is ', min_samples)\n",
    "    return trimmed_df\n",
    "\n",
    "max_samples=500 # since each class has more than 200 images all classes will be trimmed to have 200 images per class\n",
    "min_samples=173\n",
    "column='labels'\n",
    "train_df= trim(train_df, max_samples, min_samples, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b12e98",
   "metadata": {},
   "source": [
    "#### Balancing train_df so each class has 500 images using augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceebaceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial length of dataframe is  1173\n",
      "Found 173 validated image filenames.     for class             Severe             creating  327  augmented images \n",
      "Total Augmented images created=  327\n",
      "Length of augmented dataframe is now  1500\n"
     ]
    }
   ],
   "source": [
    "def balance(df, n, working_dir, img_size):\n",
    "    df=df.copy()\n",
    "    print('Initial length of dataframe is ', len(df))\n",
    "    aug_dir=os.path.join(working_dir, 'aug')# directory to store augmented images\n",
    "    if os.path.isdir(aug_dir):# start with an empty directory\n",
    "        shutil.rmtree(aug_dir)\n",
    "    os.mkdir(aug_dir)        \n",
    "    for label in df['labels'].unique():    \n",
    "        dir_path=os.path.join(aug_dir,label)    \n",
    "        os.mkdir(dir_path) # make class directories within aug directory\n",
    "    # create and store the augmented images  \n",
    "    total=0\n",
    "    gen=ImageDataGenerator(horizontal_flip=True,  rotation_range=20, width_shift_range=.2,\n",
    "                                  height_shift_range=.2, zoom_range=.2)\n",
    "    groups=df.groupby('labels') # group by class\n",
    "    for label in df['labels'].unique():  # for every class               \n",
    "        group=groups.get_group(label)  # a dataframe holding only rows with the specified label \n",
    "        sample_count=len(group)   # determine how many samples there are in this class  \n",
    "        if sample_count< n: # if the class has less than target number of images\n",
    "            aug_img_count=0\n",
    "            delta=n - sample_count  # number of augmented images to create\n",
    "            target_dir=os.path.join(aug_dir, label)  # define where to write the images\n",
    "            msg='{0:40s} for class {1:^30s} creating {2:^5s} augmented images'.format(' ', label, str(delta))\n",
    "            print(msg, '\\r', end='') # prints over on the same line\n",
    "            aug_gen=gen.flow_from_dataframe( group,  x_col='filepaths', y_col=None, target_size=img_size,\n",
    "                                            class_mode=None, batch_size=1, shuffle=False, \n",
    "                                            save_to_dir=target_dir, save_prefix='aug-', color_mode='rgb',\n",
    "                                            save_format='jpg')\n",
    "            while aug_img_count<delta:\n",
    "                images=next(aug_gen)            \n",
    "                aug_img_count += len(images)\n",
    "            total +=aug_img_count\n",
    "    print('Total Augmented images created= ', total)\n",
    "    # create aug_df and merge with train_df to create composite training set ndf\n",
    "    aug_fpaths=[]\n",
    "    aug_labels=[]\n",
    "    classlist=os.listdir(aug_dir)\n",
    "    for klass in classlist:\n",
    "        classpath=os.path.join(aug_dir, klass)     \n",
    "        flist=os.listdir(classpath)    \n",
    "        for f in flist:        \n",
    "            fpath=os.path.join(classpath,f)         \n",
    "            aug_fpaths.append(fpath)\n",
    "            aug_labels.append(klass)\n",
    "    Fseries=pd.Series(aug_fpaths, name='filepaths')\n",
    "    Lseries=pd.Series(aug_labels, name='labels')\n",
    "    aug_df=pd.concat([Fseries, Lseries], axis=1)         \n",
    "    df=pd.concat([df,aug_df], axis=0).reset_index(drop=True)\n",
    "    print('Length of augmented dataframe is now ', len(df))\n",
    "    return df \n",
    "\n",
    "n=500 # number of samples in each class\n",
    "working_dir=r'./' # directory to store augmented images\n",
    "img_size=(224,224) # size of augmented images\n",
    "train_df=balance(train_df, n, working_dir, img_size)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d31b8f",
   "metadata": {},
   "source": [
    "#### Creating a custom Keras callback to continue and  set LR or halt training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd39beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_ASK(keras.callbacks.Callback):\n",
    "    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n",
    "        super(LR_ASK, self).__init__()\n",
    "        self.model=model               \n",
    "        self.ask_epoch=ask_epoch\n",
    "        self.epochs=epochs\n",
    "        self.ask=True # if True query the user on a specified epoch\n",
    "        self.lowest_vloss=np.inf\n",
    "        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "        self.best_epoch=1\n",
    "        \n",
    "        \n",
    "    def on_train_begin(self, logs=None): # this runs on the beginning of training\n",
    "        if self.ask_epoch == 0: \n",
    "            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n",
    "            self.ask_epoch=1\n",
    "        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n",
    "            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n",
    "            self.ask=False # do not query the user\n",
    "        if self.epochs == 1:\n",
    "            self.ask=False # running only for 1 epoch so do not query user\n",
    "        else:\n",
    "            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n",
    "            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')  \n",
    "        self.start_time= time.time() # set the time at which training started\n",
    "        \n",
    "    def on_train_end(self, logs=None):   # runs at the end of training  \n",
    "        print('loading model with weights from epoch ', self.best_epoch)\n",
    "        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n",
    "        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print (msg, flush=True) # print out training duration time\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n",
    "        if v_loss< self.lowest_vloss:\n",
    "            self.lowest_vloss=v_loss\n",
    "            self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "            self.best_epoch=epoch + 1\n",
    "            print (f'\\n validation loss of {v_loss:7.4f} is below lowest loss, saving weights from epoch {str(epoch + 1):3s} as best weights')\n",
    "        else:\n",
    "            print (f'\\n validation loss of {v_loss:7.4f} is above lowest loss of {self.lowest_vloss:7.4f} keeping weights from epoch {str(self.best_epoch)} as best weights')\n",
    "        \n",
    "        if self.ask: # are the conditions right to query the user?\n",
    "            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n",
    "                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n",
    "                ans=input()\n",
    "                \n",
    "                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n",
    "                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n",
    "                    self.model.stop_training = True # halt training\n",
    "                else: # user wants to continue training\n",
    "                    self.ask_epoch += int(ans)\n",
    "                    if self.ask_epoch > self.epochs:\n",
    "                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n",
    "                    else:\n",
    "                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)\n",
    "                        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "                        print(f'current LR is  {lr:7.5f}  hit enter to keep  this LR or enter a new LR')\n",
    "                        ans=input(' ')\n",
    "                        if ans =='':\n",
    "                            print (f'keeping current LR of {lr:7.5f}')\n",
    "                        else:\n",
    "                            new_lr=float(ans)\n",
    "                            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
    "                            print(' changing LR to ', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8467dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "Healthy     500\n",
      "Moderate    500\n",
      "Severe      500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (train_df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df21d0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after trimming, the maximum samples in any class is now  500  and the minimum samples in any class is  500\n"
     ]
    }
   ],
   "source": [
    "max_samples=500 \n",
    "min_samples=500\n",
    "column='labels'\n",
    "train_df= trim(train_df, max_samples, min_samples, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01358303",
   "metadata": {},
   "source": [
    "#### Creating the train_gen, test_gen final_test_gen and valid_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12a09f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 validated image filenames belonging to 3 classes.           for train generator \n",
      "Found 461 validated image filenames belonging to 3 classes.            for valid generator \n",
      "Found 913 validated image filenames belonging to 3 classes.            for test generator \n",
      "test batch size:  11   test steps:  83  number of classes :  3\n"
     ]
    }
   ],
   "source": [
    "batch_size=32 # We will use and EfficientetB3 model, with image size of (200, 250) this size should not cause resource error\n",
    "trgen=ImageDataGenerator(horizontal_flip=True,rotation_range=20 )\n",
    "t_and_v_gen=ImageDataGenerator()\n",
    "msg='{0:70s} for train generator'.format(' ')\n",
    "print(msg, '\\r', end='') # prints over on the same line\n",
    "train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "msg='{0:70s} for valid generator'.format(' ')\n",
    "print(msg, '\\r', end='') # prints over on the same line\n",
    "valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n",
    "# for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set\n",
    "# this insures that we go through all the sample in the test set exactly once.\n",
    "length=len(test_df)\n",
    "test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
    "test_steps=int(length/test_batch_size)\n",
    "msg='{0:70s} for test generator'.format(' ')\n",
    "print(msg, '\\r', end='') # prints over on the same line\n",
    "test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
    "# from the generator we can get information we will need later\n",
    "classes=list(train_gen.class_indices.keys())\n",
    "class_indices=list(train_gen.class_indices.values())\n",
    "class_count=len(classes)\n",
    "labels=test_gen.labels\n",
    "print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
